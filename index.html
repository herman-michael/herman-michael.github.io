<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> You R. Name </title> <meta name="author" content="You R. Name"> <meta name="description" content="Graduate Student | ETH Zurich | NVIDIA"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?df44dccb15554b4d2d173cb203488555"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://herman-michael.github.io/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/"><strong>Mayank</strong> Mittal <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">You</span> R. Name </h1> <p class="desc"></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic3-480.webp 480w,/assets/img/prof_pic3-800.webp 800w,/assets/img/prof_pic3-1400.webp 1400w," type="image/webp" sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw"> <img src="/assets/img/prof_pic3.jpg?1136e86996d34e26e44da40b4f820a52" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="prof_pic3.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div class="clearfix"> <p>I am currently a <em>final-year</em> PhD student at <a href="https://ethz.ch/en.html" rel="external nofollow noopener" target="_blank">ETH Zurich</a>, advised by <a href="http://www.rsl.ethz.ch/the-lab/people/person-detail.html?persid=121911" rel="external nofollow noopener" target="_blank">Marco Hutter</a>, and a Research Scientist at <a href="https://www.nvidia.com/en-us/research/" rel="external nofollow noopener" target="_blank">NVIDIA</a>.</p> <p>My research explores how robots can learn to reason about their bodies and surroundings to achieve more adaptive and versatile behaviors. This includes whole-body control for mobile manipulation, reinforcement learning for contact-rich tasks, and integrating multi-modal sensing to enhance decision-making.</p> <p>If you have any questions or would like to discuss ideas, feel free to reach out via <a href="mailto:mittalma@ethz.ch">email</a>!</p> <div class="post"> <div class="news"> <h2>news</h2> <table> <tr> <td class="date">Feb 6, 2025</td> <td class="announcement"> Two papers on quadrupedal mobile manipulation for <a href="https://arxiv.org/abs/2409.16048" rel="external nofollow noopener" target="_blank">whole-body end-effector pose tracking</a> and <a href="https://arxiv.org/abs/2502.01546" rel="external nofollow noopener" target="_blank">dynamic object pushing</a> accepted to <a href="https://2025.ieee-icra.org/" rel="external nofollow noopener" target="_blank">ICRA 2025</a> </td> </tr> <tr> <td class="date">Oct 18, 2024</td> <td class="announcement"> Our paper on <em>‘Guided Reinforcement Learning for Robust Multi-Contact Loco-Manipulation’</em> is accepted for an <em>oral</em> presentation at <a href="https://www.corl.org/" rel="external nofollow noopener" target="_blank">CoRL 2024</a> <img class="emoji" title=":tada:" alt=":tada:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png" height="20" width="20"> </td> </tr> <tr> <td class="date">Sep 6, 2024</td> <td class="announcement"> Our paper on <em>‘Perceptive Pedipulation with Local Collision Avoidance’</em> is accepted to <a href="https://2024.ieee-humanoids.org/" rel="external nofollow noopener" target="_blank">ICHR 2024</a> </td> </tr> <tr> <td class="date">Jun 24, 2024</td> <td class="announcement"> Gave a talk at the <a href="https://www.ias.informatik.tu-darmstadt.de/Workshop/IWIALS" rel="external nofollow noopener" target="_blank">IWIALS 2024</a> on Isaac Lab and learning robust legged mobile manipulation </td> </tr> <tr> <td class="date">Jun 15, 2024</td> <td class="announcement"> Gave a talk at the RSS <a href="https://sites.google.com/view/data-generation-for-robotics/home" rel="external nofollow noopener" target="_blank">Workshop on Data Generation for Robotics</a> on how simulation-based scaling can aid in learning robust skills </td> </tr> <tr> <td class="date">Jun 3, 2024</td> <td class="announcement"> Our work on <em>Orbit</em> has evolved into <a href="https://developer.nvidia.com/isaac/sim#isaac-lab" rel="external nofollow noopener" target="_blank">Isaac Lab</a>, which is now officially supported by NVIDIA. A huge thank to the team and collaborators to make this possible! </td> </tr> <tr> <td class="date">Feb 1, 2024</td> <td class="announcement"> Four papers (<a href="https://arxiv.org/abs/2403.04359" rel="external nofollow noopener" target="_blank">task symmetry in RL</a>, <a href="https://arxiv.org/abs/2402.10837" rel="external nofollow noopener" target="_blank">pedipulation</a>, <a href="https://arxiv.org/abs/2310.00982" rel="external nofollow noopener" target="_blank">semantic navigation</a> and <a href="https://orbit-surgical.github.io/" rel="external nofollow noopener" target="_blank">surgical benchmark</a>) accepted to <a href="https://2024.ieee-icra.org/" rel="external nofollow noopener" target="_blank">ICRA 2024</a> </td> </tr> <tr> <td class="date">Apr 20, 2023</td> <td class="announcement"> Our paper on <em>‘Orbit: A Unified Simulation Framework for Interactive Robot Learning Environments’</em> is accepted to IEEE RA-L and will be presented at <a href="https://ieee-iros.org/" rel="external nofollow noopener" target="_blank">IROS 2023</a> </td> </tr> <tr> <td class="date">Jul 1, 2022</td> <td class="announcement"> Our papers on <a href="https://arxiv.org/abs/2103.10534" rel="external nofollow noopener" target="_blank">articulated object</a> and <a href="https://arxiv.org/abs/2108.09779" rel="external nofollow noopener" target="_blank">in-hand</a> manipulation are accepted to IROS 2022 <img class="emoji" title=":robot:" alt=":robot:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f916.png" height="20" width="20"> </td> </tr> <tr> <td class="date">Jan 31, 2022</td> <td class="announcement"> Our paper on <em>‘A Collision-Free MPC for Whole-Body Dynamic Locomotion and Manipulation’</em> is accepted to <a href="https://www.icra2022.org/" rel="external nofollow noopener" target="_blank">ICRA 2022</a> </td> </tr> <tr> <td class="date">Oct 7, 2021</td> <td class="announcement"> Joined <a href="https://rsl.ethz.ch/" rel="external nofollow noopener" target="_blank">Marco Hutter’s group</a> at ETH Zurich as a PhD student </td> </tr> <tr> <td class="date">Jun 28, 2021</td> <td class="announcement"> Excited to start as a Deep Learning R&amp;D Engineer at NVIDIA! </td> </tr> <tr> <td class="date">May 18, 2020</td> <td class="announcement"> Excited to start my master thesis with <a href="http://animesh.garg.tech/" rel="external nofollow noopener" target="_blank">Animesh Garg</a> at <a href="https://pairlab.github.io/" rel="external nofollow noopener" target="_blank">PAIR Lab</a>, University of Toronto! </td> </tr> <tr> <td class="date">Jan 22, 2020</td> <td class="announcement"> Our paper on <em>‘Learning Camera Miscalibration Detection’</em> from my work at <a href="https://asl.ethz.ch/" rel="external nofollow noopener" target="_blank">Autonomous Systems Lab, ETH Zurich</a> is accepted to <a href="https://www.icra2020.org/" rel="external nofollow noopener" target="_blank">ICRA 2020</a> </td> </tr> <tr> <td class="date">Sep 1, 2019</td> <td class="announcement"> Started my internship with the Intelligent Automation team at <a href="https://nnaisense.com/" rel="external nofollow noopener" target="_blank">NNAISENSE</a>, Lugano! </td> </tr> <tr> <td class="date">Aug 2, 2019</td> <td class="announcement"> Our paper on <em>‘Vision-Based Autonomous UAV Navigation and Landing for Urban Search and Rescue’</em> from my internship at <a href="http://ais.informatik.uni-freiburg.de/index_en.php" rel="external nofollow noopener" target="_blank">Autonomous Intelligent Systems, University of Freiburg</a> is accepted to <a href="http://h2t-projects.webarchiv.kit.edu/Projects/ISRR2019/" rel="external nofollow noopener" target="_blank">ISRR 2019</a> </td> </tr> <tr> <td class="date">Jul 27, 2019</td> <td class="announcement"> I will be interning with the Intelligence Control team at <a href="https://nnaisense.com/" rel="external nofollow noopener" target="_blank">NNAISENSE</a> in Lugano, Switzerland from Septmember onwards! </td> </tr> <tr> <td class="date">Apr 16, 2019</td> <td class="announcement"> Presented a seminar on Hierarchical RL for the <a href="https://disco.ethz.ch/courses/seminar/" rel="external nofollow noopener" target="_blank">Deep RL course </a> (slides available <a href="/assets/documents/talks/HRL_Part2_Mayank.pdf">here</a>) </td> </tr> <tr> <td class="date">Nov 3, 2018</td> <td class="announcement"> Working at <a href="http://www.rsl.ethz.ch/" rel="external nofollow noopener" target="_blank">Robotic Systems Lab</a> on deploying RL algorithms into robots </td> </tr> <tr> <td class="date">Sep 17, 2018</td> <td class="announcement"> Started <a href="http://www.master-robotics.ethz.ch/" rel="external nofollow noopener" target="_blank">Masters in Robotics, Systems, and Controls (RSC)</a> at ETH Zurich </td> </tr> <tr> <td class="date">Sep 6, 2018</td> <td class="announcement"> Our paper on <em>‘Vision-based Autonomous Landing in Catastrophe-Struck Environments’</em> is accepted for IROS’18 Workshop on <a href="https://www.seas.upenn.edu/~loiannog/workshopIROS2018uav/index.html#main" rel="external nofollow noopener" target="_blank">Vision-based Drones: What’s Next?</a> </td> </tr> <tr> <td class="date">Jun 28, 2018</td> <td class="announcement"> Received <em>‘SIIC Student Innovation Award’</em> and <em>‘Sri. Binay Kumar Memorial Award’</em> at <a href="https://www.iitk.ac.in/doaa/convocation/" rel="external nofollow noopener" target="_blank">51st Convocation, IIT Kanpur</a> for my work on design and development of autonomous underwater vehicle </td> </tr> <tr> <td class="date">Apr 12, 2018</td> <td class="announcement"> Awarded <a href="http://students.iitk.ac.in/ss/home/wp-content/uploads/Appendix-G.pdf" rel="external nofollow noopener" target="_blank">Science and Technology Excellence Award</a> at IIT Kanpur </td> </tr> <tr> <td class="date">Dec 20, 2017</td> <td class="announcement"> Co-Organizing the course <a href="https://ae640a.github.io" rel="external nofollow noopener" target="_blank">Autonomous Navigation</a> at IIT Kanpur </td> </tr> <tr> <td class="date">May 5, 2017</td> <td class="announcement"> Working in this summer with <a href="http://www2.informatik.uni-freiburg.de/~burgard/" rel="external nofollow noopener" target="_blank">Prof. Wolfram Burgard</a> and <a href="http://www2.informatik.uni-freiburg.de/~valada/" rel="external nofollow noopener" target="_blank">Abhinav Valada</a> </td> </tr> <tr> <td class="date">Jan 22, 2017</td> <td class="announcement"> Awarded <a href="https://www.daad.de/deutschland/stipendium/datenbank/en/21148-scholarship-database/?daad=1&amp;detail=50015295&amp;origin=4&amp;page=1&amp;q=wise&amp;status=1&amp;subjectGrps" rel="external nofollow noopener" target="_blank">DAAD WISE</a> Scholarship for an internship in Germany </td> </tr> <tr> <td class="date">Dec 17, 2016</td> <td class="announcement"> Secured first runners-up at <a href="http://www.niot.res.in/SAVe/" rel="external nofollow noopener" target="_blank">SAVe-2017</a> competition </td> </tr> <tr> <td class="date">Jun 12, 2016</td> <td class="announcement"> Selected for Boeing’s Univeristy Relation Program, <a href="http://www.iitk.ac.in/dord/boeing/public/" rel="external nofollow noopener" target="_blank"><em>Abhyast</em></a> </td> </tr> <tr> <td class="date">Jan 15, 2016</td> <td class="announcement"> A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20"> </td> </tr> <tr> <td class="date">Nov 7, 2015</td> <td class="announcement"> <a class="news-title" href="/news/announcement_2/">A long announcement with details</a> </td> </tr> <tr> <td class="date">Oct 22, 2015</td> <td class="announcement"> A simple inline announcement. </td> </tr> </table> </div> </div> <hr> <h2 id="publications"><strong>selected publications</strong></h2> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">whole-body-ee-pose</abbr> </div> <div id="portela2024whole" class="col-sm-8"> <div class="title">Whole-Body End-Effector Pose Tracking</div> <div class="author"> Tifanny Portela, Andrei Cramariuc, Mayank Mittal, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Marco Hutter' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In ICRA</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2409.16048" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="abstract hidden"> <p>Combining manipulation with the mobility of legged robots is essential for a wide range of robotic applications. However, integrating an arm with a mobile base significantly increases the system’s complexity, making precise end-effector control challenging. Existing model-based approaches are often constrained by their modeling assumptions, leading to limited robustness. Meanwhile, recent Reinforcement Learning (RL) implementations restrict the arm’s workspace to be in front of the robot or track only the position to obtain decent tracking accuracy. In this work, we address these limitations by introducing a whole-body RL formulation for end-effector pose tracking in a large workspace on rough, unstructured terrains. Our proposed method involves a terrain-aware sampling strategy for the robot’s initial configuration and end-effector pose commands, as well as a game-based curriculum to extend the robot’s operating range. We validate our approach on the ANYmal quadrupedal robot with a six DoF robotic arm. Through our experiments, we show that the learned controller achieves precise command tracking over a large workspace and adapts across varying terrains such as stairs and slopes. On deployment, it achieves a pose-tracking error of 2.64 cm and 3.64 degrees, outperforming existing competitive baselines.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">dynamic-object-push</abbr> </div> <div id="dadiotis2025dynamic" class="col-sm-8"> <div class="title">Dynamic Object Goal Pushing with Mobile Manipulators through Model-Free Constrained Reinforcement Learning</div> <div class="author"> Ioannis Dadiotis, Mayank Mittal, Nikos Tsagarakis, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Marco Hutter' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In ICRA</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2502.01546" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="abstract hidden"> <p>Non-prehensile pushing to move and reorient objects to a goal is a versatile loco-manipulation skill. In the real world, the object’s physical properties and friction with the floor contain significant uncertainties, which makes the task challenging for a mobile manipulator. In this paper, we develop a learning-based controller for a mobile manipulator to move an unknown object to a desired position and yaw orientation through a sequence of pushing actions. The proposed controller for the robotic arm and the mobile base motion is trained using a constrained Reinforcement Learning (RL) formulation. We demonstrate its capability in experiments with a quadrupedal robot equipped with an arm. The learned policy achieves a success rate of 91.35% in simulation and at least 80% on hardware in challenging scenarios. Through our extensive hardware experiments, we show that the approach demonstrates high robustness against unknown objects of different masses, materials, sizes, and shapes. It reactively discovers the pushing location and direction, thus achieving contact-rich behavior while observing only the pose of the object. Additionally, we demonstrate the adaptive behavior of the learned policy towards preventing the object from toppling.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">guided-rl-mcp</abbr> </div> <div id="sleiman2024guided" class="col-sm-8"> <div class="title">Guided Reinforcement Learning for Robust Multi-Contact Loco-Manipulation</div> <div class="author"> Jean-Pierre Sleiman<sup>*</sup>, Mayank Mittal<sup>*</sup>, and Marco Hutter </div> <div class="periodical"> <em>In CoRL</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Awarded</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2410.13817" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://leggedrobotics.github.io/guided-rl-locoma" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Oral (top 6%)</p> </div> <div class="abstract hidden"> <p>Reinforcement learning (RL) often necessitates a meticulous Markov Decision Process (MDP) design tailored to each task. This work aims to address this challenge by proposing a systematic approach to behavior synthesis and control for multi-contact loco-manipulation tasks, such as navigating spring-loaded doors and manipulating heavy dishwashers. We define a task-independent MDP to train RL policies using only a single demonstration per task generated from a model-based trajectory optimizer. Our approach incorporates an adaptive phase dynamics formulation to robustly track the demonstrations while accommodating dynamic uncertainties and external disturbances. We compare our method against prior motion imitation RL works and show that the learned policies achieve higher success rates across all considered tasks. These policies learn recovery maneuvers that are not present in the demonstration, such as re-grasping objects during execution or dealing with slippages. Finally, we successfully transfer the policies to a real robot, demonstrating the practical viability of our approach.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">perceptive-pedipulate</abbr> </div> <div id="stolle2024perceptivepedipulate" class="col-sm-8"> <div class="title">Perceptive Pedipulation with Local Obstacle Avoidance</div> <div class="author"> Jonas Stolle, Philip Arm, Mayank Mittal, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Marco Hutter' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In ICHR</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Awarded</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2409.07195" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://sites.google.com/leggedrobotics.com/perceptive-pedipulation" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Best Interactive Poster Finalist</p> </div> <div class="abstract hidden"> <p>Pedipulation leverages the feet of legged robots for mobile manipulation, eliminating the need for dedicated robotic arms. While previous works have showcased blind and task-specific pedipulation skills, they fail to account for static and dynamic obstacles in the environment. To address this limitation, we introduce a reinforcement learning-based approach to train a whole-body obstacle-aware policy that tracks foot position commands while simultaneously avoiding obstacles. Despite training the policy in only five different static scenarios in simulation, we show that it generalizes to unknown environments with different numbers and types of obstacles. We analyze the performance of our method through a set of simulation experiments and successfully deploy the learned policy on the ANYmal quadruped, demonstrating its capability to follow foot commands while navigating around static and dynamic obstacles.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">symmetry</abbr> </div> <div id="mittal2024symmetry" class="col-sm-8"> <div class="title">Symmetry Considerations for Learning Task Symmetric Robot Policies</div> <div class="author"> Mayank Mittal<sup>*</sup>, Nikita Rudin<sup>*</sup>, Victor Klemm, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Arthur Allshire, Marco Hutter' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In ICRA</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2403.04359" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/leggedrobotics/rsl_rl" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Symmetry is a fundamental aspect of many real-world robotic tasks. However, current deep reinforcement learning (DRL) approaches can seldom harness and exploit symmetry effectively. Often, the learned behaviors fail to achieve the desired transformation invariances and suffer from motion artifacts. For instance, a quadruped may exhibit different gaits when commanded to move forward or backward, even though it is symmetrical about its torso. This issue becomes further pronounced in high-dimensional or complex environments, where DRL methods are prone to local optima and fail to explore regions of the state space equally. Past methods on encouraging symmetry for robotic tasks have studied this topic mainly in a single-task setting, where symmetry usually refers to symmetry in the motion, such as the gait patterns. In this paper, we revisit this topic for goal-conditioned tasks in robotics, where symmetry lies mainly in task execution and not necessarily in the learned motions themselves. In particular, we investigate two approaches to incorporate symmetry invariance into DRL – data augmentation and mirror loss function. We provide a theoretical foundation for using augmented samples in an on-policy setting. Based on this, we show that the corresponding approach achieves faster convergence and improves the learned behaviors in various challenging robotic tasks, from climbing boxes with a quadruped to dexterous manipulation.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">pedipulate</abbr> </div> <div id="arm2024pedipulate" class="col-sm-8"> <div class="title">Pedipulate: Enabling Manipulation Skills using a Quadruped Robot’s Leg</div> <div class="author"> Philip Arm, Mayank Mittal, Hendrik Kolvenbach, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Marco Hutter' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In ICRA</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2402.10837" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://sites.google.com/leggedrobotics.com/pedipulate" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://youtu.be/GD4WyJPXQtU" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>Legged robots have the potential to become vital in maintenance, home support, and exploration scenarios. In order to interact with and manipulate their environments, most legged robots are equipped with a dedicated robot arm, which means additional mass and mechanical complexity compared to standard legged robots. In this work, we explore pedipulation - using the legs of a legged robot for manipulation. By training a reinforcement learning policy that tracks position targets for one foot, we enable a dedicated pedipulation controller that is robust to disturbances, has a large workspace through whole-body behaviors, and can reach far-away targets with gait emergence, enabling loco-pedipulation. By deploying our controller on a quadrupedal robot using teleoperation, we demonstrate various real-world tasks such as door opening, sample collection, and pushing obstacles. We demonstrate load carrying of more than 2.0 kg at the foot. Additionally, the controller is robust to interaction forces at the foot, disturbances at the base, and slippery contact surfaces.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">orbit</abbr> </div> <div id="mittal2023orbit" class="col-sm-8"> <div class="title">ORBIT: A Unified Simulation Framework for Interactive Robot Learning Environments</div> <div class="author"> Mayank Mittal, Calvin Yu, Qinxi Yu, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Jingzhou Liu, Nikita Rudin, David Hoeller, others' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>In IEEE RA-L</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Awarded</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2301.04195" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://isaac-orbit.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/NVIDIA-Omniverse/orbit" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>renamed to NVIDIA Isaac Lab</p> </div> <div class="abstract hidden"> <p>We present ORBIT, a unified and modular framework for robot learning powered by NVIDIA Isaac Sim. It offers a modular design to easily and efficiently create robotic environments with photo-realistic scenes and fast and accurate rigid and deformable body simulation. With ORBIT, we provide a suite of benchmark tasks of varying difficulty – from single-stage cabinet opening and cloth folding to multi-stage tasks such as room reorganization. To support working with diverse observations and action spaces, we include fixed-arm and mobile manipulators with different physically-based sensors and motion generators. ORBIT allows training reinforcement learning policies and collecting large demonstration datasets from hand-crafted or expert solutions in a matter of minutes by leveraging GPU-based parallelization. In summary, we offer an open-sourced framework that readily comes with 16 robotic platforms, 4 sensor modalities, 10 motion generators, more than 20 benchmark tasks, and wrappers to 4 learning libraries. With this framework, we aim to support various research areas, including representation learning, reinforcement learning, imitation learning, and task and motion planning. We hope it helps establish interdisciplinary collaborations in these communities, and its modularity makes it easily extensible for more tasks and applications in the future.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">alma_self_collision</abbr> </div> <div id="2202.12385" class="col-sm-8"> <div class="title">A Collision-Free MPC for Whole-Body Dynamic Locomotion and Manipulation</div> <div class="author"> Jia-Ruei Chiu, Jean-Pierre Sleiman, Mayank Mittal, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Farbod Farshidian, Marco Hutter' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In ICRA</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2202.12385" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://www.youtube.com/watch?v=m3rJWJVzYuY" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/leggedrobotics/ocs2" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>In this paper, we present a real-time whole-body planner for collision-free legged mobile manipulation. We enforce both self-collision and environment-collision avoidance as soft constraints within a Model Predictive Control (MPC) scheme that solves a multi-contact optimal control problem. By penalizing the signed distances among a set of representative primitive collision bodies, the robot is able to safely execute a variety of dynamic maneuvers while preventing any self-collisions. Moreover, collision-free navigation and manipulation in both static and dynamic environments are made viable through efficient queries of distances and their gradients via a euclidean signed distance field. We demonstrate through a comparative study that our approach only slightly increases the computational complexity of the MPC planning. Finally, we validate the effectiveness of our framework through a set of hardware experiments involving dynamic mobile manipulation tasks with potential collisions, such as locomotion balancing with the swinging arm, weight throwing, and autonomous door opening.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">articulated_mm</abbr> </div> <div id="2002.10451" class="col-sm-8"> <div class="title">Articulated Object Interaction in Unknown Scenes with Whole-Body Mobile Manipulation</div> <div class="author"> Mayank Mittal, David Hoeller, Farbod Farshidian, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Marco Hutter, Animesh Garg' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In IROS</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2103.10534" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://www.pair.toronto.edu/articulated-mm/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>A kitchen assistant needs to operate human-scale objects, such as cabinets and ovens, in unmapped environments with dynamic obstacles. Autonomous interactions in such real-world environments require integrating dexterous manipulation and fluid mobility. While mobile manipulators in different form-factors provide an extended workspace, their real-world adoption has been limited. This limitation is in part due to two main reasons: 1) inability to interact with unknown human-scale objects such as cabinets and ovens, and 2) inefficient coordination between the arm and the mobile base. Executing a high-level task for general objects requires a perceptual understanding of the object as well as adaptive whole-body control among dynamic obstacles. In this paper, we propose a two-stage architecture for autonomous interaction with large articulated objects in unknown environments. The first stage uses a learned model to estimate the articulated model of a target object from an RGB-D input and predicts an action-conditional sequence of states for interaction. The second stage comprises of a whole-body motion controller to manipulate the object along the generated kinematic plan. We show that our proposed pipeline can handle complicated static and dynamic kitchen settings. Moreover, we demonstrate that the proposed approach achieves better performance than commonly used control methods in mobile manipulation.</p> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%79%6F%75@%65%78%61%6D%70%6C%65.%63%6F%6D" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://inspirehep.net/authors/1010907" title="Inspire HEP" rel="external nofollow noopener" target="_blank"><i class="ai ai-inspire"></i></a> <a href="/feed.xml" title="RSS Feed"><i class="fa-solid fa-square-rss"></i></a> <a href="https://scholar.google.com/citations?user=qc6CJjYAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.alberteinstein.com/" title="Custom Social" rel="external nofollow noopener" target="_blank"> <img src="https://www.alberteinstein.com/wp-content/uploads/2024/03/cropped-favicon-192x192.png" alt="Custom Social"> </a> </div> <div class="contact-note">You can even add a little note about which of these is the best way to reach you. </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 You R. Name. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?79751be30bd885c9c249a026e6dd1424"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>